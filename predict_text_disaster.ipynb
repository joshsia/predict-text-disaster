{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting whether tweets refer to a disaster or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "import os\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, RidgeCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    ShuffleSplit,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split\n",
    ")\n",
    "from scipy.stats import loguniform, randint\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    PolynomialFeatures,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Aim\n",
    "\n",
    "The aim of this problem is to predict whether a tweet refers to a disaster event or not. This is a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data\n",
    "\n",
    "The data used in this project can be found publicly on [Kaggle Disaster Tweets](https://www.kaggle.com/vstepanenko/disaster-tweets). The data set contains the tweet as text data, keywords found in the tweet, the location where the tweet was made, and the target describing whether the tweet refers to a disaster (`target=1`) or not (`target=0`).\n",
    "\n",
    "In the next steps, the code assumes that the data file is stored as `tweets.csv` in the project root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. EDA\n",
    "\n",
    "Before building any models, let us have a look at the data set more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>debris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unfortunately, both plans fail as the 3 are im...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>crash</td>\n",
       "      <td>SLC</td>\n",
       "      <td>I hope this causes Bernie to crash and bern. S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>collide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>—pushes himself up from the chair beneath to r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9622</th>\n",
       "      <td>suicide%20bomb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Widow of CIA agent killed in 2009 Afghanistan ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>screaming</td>\n",
       "      <td>Azania</td>\n",
       "      <td>As soon as God say yes they'll be screaming we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             keyword location  \\\n",
       "3289          debris      NaN   \n",
       "2672           crash      SLC   \n",
       "2436         collide      NaN   \n",
       "9622  suicide%20bomb      NaN   \n",
       "8999       screaming   Azania   \n",
       "\n",
       "                                                   text  target  \n",
       "3289  Unfortunately, both plans fail as the 3 are im...       0  \n",
       "2672  I hope this causes Bernie to crash and bern. S...       0  \n",
       "2436  —pushes himself up from the chair beneath to r...       0  \n",
       "9622  Widow of CIA agent killed in 2009 Afghanistan ...       1  \n",
       "8999  As soon as God say yes they'll be screaming we...       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"tweets.csv\", usecols=[\"keyword\", \"text\", \"target\", \"location\"]\n",
    ")\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=2\n",
    ")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df.drop(columns=[\"target\"]), train_df[\"target\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"target\"]), test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.812995\n",
       "1    0.187005\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"target\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Rudimentary EDA reveals that there is **class imbalance** since only around 19% of the examples in the training set belong to the \"True\" class, which is our interest. If we do not deal with class imbalance, results can be misleading, for instance, if 99% of examples have the label False, a DummyClassifier will have an accuracy of 99%. This does not mean that the dummy model is good, because it will likely have poor recall and precision.\n",
    "\n",
    "To deal with class imbalance, metrics that are relevant to classification problems, namely **accuracy, precision, recall, f1-score, ROC AUC, and average prediction** are used. \n",
    "\n",
    "In this problem, a false positive corresponds to a tweet being classified as referring to a disaster, when it actually isn't. A false negative corresponds to a tweet being classified as not referring to a disaster, when it actually is. A false negative is more harmful in this case and thus, we want to minimise the number of false negatives, which is equivalent to increasing the recall. For this reason, among all the scoring metrics, the accuracy and precision will be taken less into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_metrics = [\"accuracy\", \"precision\", \"recall\",\n",
    "                   \"f1\", \"roc_auc\", \"average_precision\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Further EDA reveals that the `location` column contains a large proportion of **missing values** which will make it challenging for imputation. Furthermore, some of the values are **not even real locations**. Thus, in this project, the feature `location` will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of NA values in location feature: 0.2997\n"
     ]
    }
   ],
   "source": [
    "print(f\"Proportion of NA values in location feature: \"\n",
    "      f\"{round(X_train['location'].isna().sum() / len(X_train.index), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>Rwanda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11099</th>\n",
       "      <td>Norwich, UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2581</th>\n",
       "      <td>Dublin, Ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9022</th>\n",
       "      <td>bi 18 she/her</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11237</th>\n",
       "      <td>대한민국 서울</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11053</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10483</th>\n",
       "      <td>Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6820</th>\n",
       "      <td>Louisiana, USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10763</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6766</th>\n",
       "      <td>Birmingham, England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>New york in jesus name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11230</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     location\n",
       "5998                   Rwanda\n",
       "11099             Norwich, UK\n",
       "2581          Dublin, Ireland\n",
       "9022            bi 18 she/her\n",
       "11237                 대한민국 서울\n",
       "11053                     NaN\n",
       "10483               Australia\n",
       "6820           Louisiana, USA\n",
       "10763                     NaN\n",
       "6766      Birmingham, England\n",
       "229    New york in jesus name\n",
       "11230                     NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[[\"location\"]].sample(n=12, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = [\"location\"]\n",
    "text_features = \"keyword\"\n",
    "text_features2 = \"text\"\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (CountVectorizer(stop_words=\"english\"), text_features),\n",
    "    (CountVectorizer(stop_words=\"english\"), text_features2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `location` feature is dropped because there are a lot of missing values. The `text` and `keyword` features are transformed using the **bag of words representation** since the features are free text, and there are no fixed number of categories for both features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.001 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.004 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.813 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_roc_auc</th>\n",
       "      <td>0.500 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_average_precision</th>\n",
       "      <td>0.187 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    dummy\n",
       "fit_time                0.001 (+/- 0.000)\n",
       "score_time              0.004 (+/- 0.001)\n",
       "test_accuracy           0.813 (+/- 0.000)\n",
       "test_precision          0.000 (+/- 0.000)\n",
       "test_recall             0.000 (+/- 0.000)\n",
       "test_f1                 0.000 (+/- 0.000)\n",
       "test_roc_auc            0.500 (+/- 0.000)\n",
       "test_average_precision  0.187 (+/- 0.000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "results[\"dummy\"] = mean_std_cross_val_scores(\n",
    "    DummyClassifier(), X_train, y_train, scoring=scoring_metrics\n",
    ")\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the dummy classifier is 0.813 since this corresponds to the proportion of the most common label. There are warnings when trying to compute the precision because the dummy classifier will never predict the class True (it always predicts False since that is the most common class), and thus, when computing precision, the number of false positives is zero, and there will be division by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "      <th>lr_default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.001 (+/- 0.000)</td>\n",
       "      <td>0.291 (+/- 0.019)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.004 (+/- 0.001)</td>\n",
       "      <td>0.056 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.813 (+/- 0.000)</td>\n",
       "      <td>0.887 (+/- 0.005)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "      <td>0.811 (+/- 0.012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "      <td>0.513 (+/- 0.036)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "      <td>0.628 (+/- 0.026)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_roc_auc</th>\n",
       "      <td>0.500 (+/- 0.000)</td>\n",
       "      <td>0.898 (+/- 0.011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_average_precision</th>\n",
       "      <td>0.187 (+/- 0.000)</td>\n",
       "      <td>0.747 (+/- 0.018)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    dummy         lr_default\n",
       "fit_time                0.001 (+/- 0.000)  0.291 (+/- 0.019)\n",
       "score_time              0.004 (+/- 0.001)  0.056 (+/- 0.001)\n",
       "test_accuracy           0.813 (+/- 0.000)  0.887 (+/- 0.005)\n",
       "test_precision          0.000 (+/- 0.000)  0.811 (+/- 0.012)\n",
       "test_recall             0.000 (+/- 0.000)  0.513 (+/- 0.036)\n",
       "test_f1                 0.000 (+/- 0.000)  0.628 (+/- 0.026)\n",
       "test_roc_auc            0.500 (+/- 0.000)  0.898 (+/- 0.011)\n",
       "test_average_precision  0.187 (+/- 0.000)  0.747 (+/- 0.018)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_default_pipe = make_pipeline(\n",
    "    preprocessor, LogisticRegression(random_state=123)\n",
    ")\n",
    "\n",
    "results[\"lr_default\"] = mean_std_cross_val_scores(\n",
    "    lr_default_pipe, X_train, y_train, scoring=scoring_metrics\n",
    ")\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression classifier **performed better** than the dummy classifier on all scoring metrics, which is a good sign. However, the **recall seems to be a bit low**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_pipe = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegression(max_iter=2000, random_state=123)\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"logisticregression__C\": loguniform(1e-2, 1e4),\n",
    "    \"logisticregression__class_weight\": [\"balanced\", None],\n",
    "    \"columntransformer__countvectorizer-1__max_features\": randint(low=50,\n",
    "                                                                  high=250),\n",
    "    \"columntransformer__countvectorizer-2__max_features\": randint(low=5_000,\n",
    "                                                                  high=24_000)\n",
    "}\n",
    "\n",
    "random_search_lr = RandomizedSearchCV(\n",
    "    lr_pipe,\n",
    "    param_distributions=param_grid,\n",
    "    scoring=scoring_metrics,\n",
    "    refit=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    n_iter=200,\n",
    "    cv=5,\n",
    "    random_state=123,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "random_search_lr.fit(X_train, y_train)\n",
    "\n",
    "results_rs = pd.DataFrame(random_search_lr.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>mean_test_average_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>param_columntransformer__countvectorizer-1__max_features</th>\n",
       "      <th>param_columntransformer__countvectorizer-2__max_features</th>\n",
       "      <th>param_logisticregression__C</th>\n",
       "      <th>param_logisticregression__class_weight</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.676655</td>\n",
       "      <td>0.671703</td>\n",
       "      <td>0.743387</td>\n",
       "      <td>0.667566</td>\n",
       "      <td>0.876649</td>\n",
       "      <td>223</td>\n",
       "      <td>20893</td>\n",
       "      <td>0.439014</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.386503</td>\n",
       "      <td>0.093603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.897748</td>\n",
       "      <td>0.665491</td>\n",
       "      <td>0.669870</td>\n",
       "      <td>0.742367</td>\n",
       "      <td>0.674929</td>\n",
       "      <td>0.877639</td>\n",
       "      <td>139</td>\n",
       "      <td>20132</td>\n",
       "      <td>0.574004</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.479325</td>\n",
       "      <td>0.104191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.897748</td>\n",
       "      <td>0.482060</td>\n",
       "      <td>0.606468</td>\n",
       "      <td>0.743722</td>\n",
       "      <td>0.819892</td>\n",
       "      <td>0.883245</td>\n",
       "      <td>157</td>\n",
       "      <td>17264</td>\n",
       "      <td>0.583232</td>\n",
       "      <td>None</td>\n",
       "      <td>0.450844</td>\n",
       "      <td>0.107342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.897667</td>\n",
       "      <td>0.674309</td>\n",
       "      <td>0.670205</td>\n",
       "      <td>0.742849</td>\n",
       "      <td>0.666732</td>\n",
       "      <td>0.876209</td>\n",
       "      <td>198</td>\n",
       "      <td>18805</td>\n",
       "      <td>0.488096</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.452970</td>\n",
       "      <td>0.101518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.897637</td>\n",
       "      <td>0.470307</td>\n",
       "      <td>0.598939</td>\n",
       "      <td>0.745440</td>\n",
       "      <td>0.826116</td>\n",
       "      <td>0.882366</td>\n",
       "      <td>246</td>\n",
       "      <td>21103</td>\n",
       "      <td>0.465738</td>\n",
       "      <td>None</td>\n",
       "      <td>0.537057</td>\n",
       "      <td>0.134701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   mean_test_roc_auc  mean_test_recall  mean_test_f1  \\\n",
       "rank_test_roc_auc                                                      \n",
       "1                           0.897887          0.676655      0.671703   \n",
       "2                           0.897748          0.665491      0.669870   \n",
       "3                           0.897748          0.482060      0.606468   \n",
       "4                           0.897667          0.674309      0.670205   \n",
       "5                           0.897637          0.470307      0.598939   \n",
       "\n",
       "                   mean_test_average_precision  mean_test_precision  \\\n",
       "rank_test_roc_auc                                                     \n",
       "1                                     0.743387             0.667566   \n",
       "2                                     0.742367             0.674929   \n",
       "3                                     0.743722             0.819892   \n",
       "4                                     0.742849             0.666732   \n",
       "5                                     0.745440             0.826116   \n",
       "\n",
       "                   mean_test_accuracy  \\\n",
       "rank_test_roc_auc                       \n",
       "1                            0.876649   \n",
       "2                            0.877639   \n",
       "3                            0.883245   \n",
       "4                            0.876209   \n",
       "5                            0.882366   \n",
       "\n",
       "                  param_columntransformer__countvectorizer-1__max_features  \\\n",
       "rank_test_roc_auc                                                            \n",
       "1                                                                223         \n",
       "2                                                                139         \n",
       "3                                                                157         \n",
       "4                                                                198         \n",
       "5                                                                246         \n",
       "\n",
       "                  param_columntransformer__countvectorizer-2__max_features  \\\n",
       "rank_test_roc_auc                                                            \n",
       "1                                                              20893         \n",
       "2                                                              20132         \n",
       "3                                                              17264         \n",
       "4                                                              18805         \n",
       "5                                                              21103         \n",
       "\n",
       "                  param_logisticregression__C  \\\n",
       "rank_test_roc_auc                               \n",
       "1                                    0.439014   \n",
       "2                                    0.574004   \n",
       "3                                    0.583232   \n",
       "4                                    0.488096   \n",
       "5                                    0.465738   \n",
       "\n",
       "                  param_logisticregression__class_weight  mean_fit_time  \\\n",
       "rank_test_roc_auc                                                         \n",
       "1                                               balanced       0.386503   \n",
       "2                                               balanced       0.479325   \n",
       "3                                                   None       0.450844   \n",
       "4                                               balanced       0.452970   \n",
       "5                                                   None       0.537057   \n",
       "\n",
       "                   mean_score_time  \n",
       "rank_test_roc_auc                   \n",
       "1                         0.093603  \n",
       "2                         0.104191  \n",
       "3                         0.107342  \n",
       "4                         0.101518  \n",
       "5                         0.134701  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"mean_test_roc_auc\", \"mean_test_recall\",\n",
    "           \"mean_test_f1\", \"mean_test_average_precision\",\n",
    "           \"mean_test_precision\", \"mean_test_accuracy\",\n",
    "           \"param_columntransformer__countvectorizer-1__max_features\",\n",
    "           \"param_columntransformer__countvectorizer-2__max_features\",\n",
    "           \"param_logisticregression__C\",\n",
    "           \"param_logisticregression__class_weight\",\n",
    "           \"mean_fit_time\", \"mean_score_time\"]\n",
    "\n",
    "ranked_results = (results_rs.set_index(\"rank_test_roc_auc\")\n",
    "                  .sort_index()[columns])\n",
    "\n",
    "results[\"lr_hyp_opt\"] = [\n",
    "    ranked_results.iloc[1][\"mean_fit_time\"],\n",
    "    ranked_results.iloc[1][\"mean_score_time\"],\n",
    "    ranked_results.iloc[1][\"mean_test_accuracy\"],\n",
    "    ranked_results.iloc[1][\"mean_test_precision\"],\n",
    "    ranked_results.iloc[1][\"mean_test_recall\"],\n",
    "    ranked_results.iloc[1][\"mean_test_f1\"],\n",
    "    ranked_results.iloc[1][\"mean_test_roc_auc\"],\n",
    "    ranked_results.iloc[1][\"mean_test_average_precision\"]\n",
    "]\n",
    "\n",
    "ranked_results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'columntransformer__countvectorizer-1__max_features': 223, 'columntransformer__countvectorizer-2__max_features': 20893, 'logisticregression__C': 0.43901433023426095, 'logisticregression__class_weight': 'balanced'}\n",
      "0.8978865079428952\n"
     ]
    }
   ],
   "source": [
    "print(random_search_lr.best_params_)\n",
    "print(random_search_lr.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The best hyperparameter values are `max_features = 223` for the keyword feature, `max_features = 20893` for the text feature, `C = 0.439` and `class_weight=\"balanced\"` for the logistic regression. The best cross-validation ROC AUC score found with these hyperparameter values was 0.898."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature engineering\n",
    "\n",
    "Now, let us explore whether we can engineer new features which our model find useful in classifying the tweets.\n",
    "\n",
    "Several basic length-related and sentiment features are engineered such as:\n",
    "- Relative character length.\n",
    "- Number of words.\n",
    "- Sentiment of the tweet.\n",
    "- Number of nouns.\n",
    "- Number of proper nouns.\n",
    "- Whether the tweet contains a number.\n",
    "\n",
    "The metric used to measure the sentiment is the compound score in which a score of -1 corresponds to extremely negative and a score of +1 corresponds to extremely positive. This score is extracted using [Vader lexicon](https://github.com/cjhutto/vaderSentiment).\n",
    "\n",
    "The number of nouns and proper nouns in the text which could be useful because a tweet that refers to a disaster is likely to include many nouns describing the location or time (e.g. Canada, park, Friday, etc.), whereas a tweet that does not refer to a disaster might not have as many nouns.\n",
    "\n",
    "On the other hand, whether the tweet contains a number could be useful because tweets referring to an actual disaster might include numbers such as the year, or number of casualties etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_length(text, TWITTER_ALLOWED_CHARS=280.0):\n",
    "    \"\"\"\n",
    "    Returns the relative length of text.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------\n",
    "    TWITTER_ALLOWED_CHARS: (float)\n",
    "    the denominator for finding relative length\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    relative length of text: (float)\n",
    "\n",
    "    \"\"\"\n",
    "    return len(text) / TWITTER_ALLOWED_CHARS\n",
    "\n",
    "\n",
    "def get_length_in_words(text):\n",
    "    \"\"\"\n",
    "    Returns the length of the text in words.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    length of tokenized text: (int)\n",
    "\n",
    "    \"\"\"\n",
    "    return len(nltk.word_tokenize(text))\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Returns the compound score representing the sentiment of the given text: -1 (most extreme negative) and +1 (most extreme positive)\n",
    "    The compound score is a normalized score calculated by summing the valence scores of each word in the lexicon.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    sentiment of the text: (str)\n",
    "    \"\"\"\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores[\"compound\"]\n",
    "\n",
    "def get_number_of_nouns(text):\n",
    "    \"\"\"\n",
    "    Returns the number of nouns in the text.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    number of nouns: (int)\n",
    "\n",
    "    \"\"\"\n",
    "    tags = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    return sum([1 if _[1] in [\"NN\", \"NNP\", \"NNS\", \"NNPS\"]\n",
    "                else 0 for _ in tags])\n",
    "\n",
    "\n",
    "def get_number_of_proper_nouns(text):\n",
    "    \"\"\"\n",
    "    Returns the number of proper nouns in the text.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    number of proper nouns: (int)\n",
    "\n",
    "    \"\"\"\n",
    "    tags = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    return sum([1 if _[1] in [\"NNP\", \"NNPS\"]\n",
    "                else 0 for _ in tags])\n",
    "\n",
    "\n",
    "def has_numbers(text):\n",
    "    \"\"\"\n",
    "    Returns whether the text has numbers or not.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    whether text has numbers: (bool)\n",
    "\n",
    "    \"\"\"\n",
    "    return 1 if any(char.isdigit() for char in text) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.assign(\n",
    "    n_words=train_df[\"text\"].apply(get_length_in_words))\n",
    "train_df = train_df.assign(\n",
    "    vader_sentiment=train_df[\"text\"].apply(get_sentiment))\n",
    "train_df = train_df.assign(\n",
    "    rel_char_len=train_df[\"text\"].apply(get_relative_length))\n",
    "train_df = train_df.assign(\n",
    "    n_nouns=train_df[\"text\"].apply(get_number_of_nouns))\n",
    "train_df = train_df.assign(\n",
    "    n_proper_nouns=train_df[\"text\"].apply(get_number_of_proper_nouns))\n",
    "train_df = train_df.assign(\n",
    "    has_number=train_df[\"text\"].apply(has_numbers))\n",
    "\n",
    "test_df = test_df.assign(\n",
    "    n_words=test_df[\"text\"].apply(get_length_in_words))\n",
    "test_df = test_df.assign(\n",
    "    vader_sentiment=test_df[\"text\"].apply(get_sentiment))\n",
    "test_df = test_df.assign(\n",
    "    rel_char_len=test_df[\"text\"].apply(get_relative_length))\n",
    "test_df = test_df.assign(\n",
    "    n_nouns=test_df[\"text\"].apply(get_number_of_nouns))\n",
    "test_df = test_df.assign(\n",
    "    n_proper_nouns=test_df[\"text\"].apply(get_number_of_proper_nouns))\n",
    "test_df = test_df.assign(\n",
    "    has_number=test_df[\"text\"].apply(has_numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pipeline with engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eng, y_train_eng = train_df.drop(columns=[\"target\"]), train_df[\"target\"]\n",
    "X_test_eng, y_test_eng = test_df.drop(columns=[\"target\"]), test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "      <th>lr_default</th>\n",
       "      <th>lr_hyp_opt</th>\n",
       "      <th>lr_feat_eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.001 (+/- 0.000)</td>\n",
       "      <td>0.291 (+/- 0.019)</td>\n",
       "      <td>0.479325</td>\n",
       "      <td>0.418 (+/- 0.057)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.004 (+/- 0.001)</td>\n",
       "      <td>0.056 (+/- 0.001)</td>\n",
       "      <td>0.104191</td>\n",
       "      <td>0.062 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.813 (+/- 0.000)</td>\n",
       "      <td>0.887 (+/- 0.005)</td>\n",
       "      <td>0.877639</td>\n",
       "      <td>0.876 (+/- 0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "      <td>0.811 (+/- 0.012)</td>\n",
       "      <td>0.674929</td>\n",
       "      <td>0.660 (+/- 0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "      <td>0.513 (+/- 0.036)</td>\n",
       "      <td>0.665491</td>\n",
       "      <td>0.700 (+/- 0.021)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "      <td>0.628 (+/- 0.026)</td>\n",
       "      <td>0.669870</td>\n",
       "      <td>0.679 (+/- 0.009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_roc_auc</th>\n",
       "      <td>0.500 (+/- 0.000)</td>\n",
       "      <td>0.898 (+/- 0.011)</td>\n",
       "      <td>0.897748</td>\n",
       "      <td>0.898 (+/- 0.011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_average_precision</th>\n",
       "      <td>0.187 (+/- 0.000)</td>\n",
       "      <td>0.747 (+/- 0.018)</td>\n",
       "      <td>0.742367</td>\n",
       "      <td>0.743 (+/- 0.017)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    dummy         lr_default  lr_hyp_opt  \\\n",
       "fit_time                0.001 (+/- 0.000)  0.291 (+/- 0.019)    0.479325   \n",
       "score_time              0.004 (+/- 0.001)  0.056 (+/- 0.001)    0.104191   \n",
       "test_accuracy           0.813 (+/- 0.000)  0.887 (+/- 0.005)    0.877639   \n",
       "test_precision          0.000 (+/- 0.000)  0.811 (+/- 0.012)    0.674929   \n",
       "test_recall             0.000 (+/- 0.000)  0.513 (+/- 0.036)    0.665491   \n",
       "test_f1                 0.000 (+/- 0.000)  0.628 (+/- 0.026)    0.669870   \n",
       "test_roc_auc            0.500 (+/- 0.000)  0.898 (+/- 0.011)    0.897748   \n",
       "test_average_precision  0.187 (+/- 0.000)  0.747 (+/- 0.018)    0.742367   \n",
       "\n",
       "                              lr_feat_eng  \n",
       "fit_time                0.418 (+/- 0.057)  \n",
       "score_time              0.062 (+/- 0.001)  \n",
       "test_accuracy           0.876 (+/- 0.002)  \n",
       "test_precision          0.660 (+/- 0.002)  \n",
       "test_recall             0.700 (+/- 0.021)  \n",
       "test_f1                 0.679 (+/- 0.009)  \n",
       "test_roc_auc            0.898 (+/- 0.011)  \n",
       "test_average_precision  0.743 (+/- 0.017)  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features = [\"n_words\", \"vader_sentiment\", \"rel_char_len\",\n",
    "                    \"n_nouns\", \"n_proper_nouns\"]\n",
    "\n",
    "passthrough_features = [\"has_number\"]\n",
    "\n",
    "text_features = \"keyword\"\n",
    "text_features2 = \"text\"\n",
    "\n",
    "drop_features = \"location\"\n",
    "\n",
    "opt_max_features1 = (\n",
    "    random_search_lr\n",
    "    .best_params_[\n",
    "        \"columntransformer__countvectorizer-1__max_features\"])\n",
    "opt_max_features2 = (\n",
    "    random_search_lr\n",
    "    .best_params_[\n",
    "        \"columntransformer__countvectorizer-2__max_features\"])\n",
    "opt_C = random_search_lr.best_params_[\"logisticregression__C\"]\n",
    "opt_class_weight = (\n",
    "    random_search_lr\n",
    "    .best_params_[\"logisticregression__class_weight\"])\n",
    "\n",
    "preprocessor_feat_eng = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (StandardScaler(), numeric_features),\n",
    "    (\"passthrough\", passthrough_features),\n",
    "    (CountVectorizer(stop_words=\"english\",\n",
    "                     max_features=opt_max_features1),\n",
    "     text_features),\n",
    "    (CountVectorizer(stop_words=\"english\",\n",
    "                     max_features=opt_max_features2),\n",
    "     text_features2)\n",
    ")\n",
    "\n",
    "pipe_lr_feat_eng = make_pipeline(\n",
    "    preprocessor_feat_eng,\n",
    "    LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        C=opt_C,\n",
    "        class_weight=opt_class_weight,\n",
    "        random_state=123)\n",
    ")\n",
    "\n",
    "results[\"lr_feat_eng\"] = mean_std_cross_val_scores(\n",
    "    pipe_lr_feat_eng, X_train_eng, y_train_eng,\n",
    "    scoring=scoring_metrics\n",
    ")\n",
    "\n",
    "pipe_lr_feat_eng.fit(X_train_eng, y_train_eng)\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The cross-validation scores are similar for the accuracy, ROC AUC and average precision. However, after feature engineering, the **recall and f-1 score increased** while the precision decreased. This is a good sign because false negatives are more harmful in this problem, and a larger recall indicates a lower number of false negatives. However, no large improvement was observed after feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>thunderstorm</th>\n",
       "      <td>1.672021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>1.550044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>windstorm</th>\n",
       "      <td>1.540446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>died</th>\n",
       "      <td>1.514138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rescued</th>\n",
       "      <td>1.484401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collision</th>\n",
       "      <td>1.388583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>road</th>\n",
       "      <td>1.337435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ukrainian</th>\n",
       "      <td>1.315033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carried</th>\n",
       "      <td>1.263005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heavy</th>\n",
       "      <td>1.212298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Coefficients\n",
       "thunderstorm      1.672021\n",
       "survived          1.550044\n",
       "windstorm         1.540446\n",
       "died              1.514138\n",
       "rescued           1.484401\n",
       "collision         1.388583\n",
       "road              1.337435\n",
       "ukrainian         1.315033\n",
       "carried           1.263005\n",
       "heavy             1.212298"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = (numeric_features + passthrough_features +\n",
    "                (pipe_lr_feat_eng.named_steps[\"columntransformer\"]\n",
    "                 .named_transformers_[\"countvectorizer-1\"]\n",
    "                 .get_feature_names_out().tolist()) +\n",
    "                (pipe_lr_feat_eng.named_steps[\"columntransformer\"]\n",
    "                 .named_transformers_[\"countvectorizer-2\"]\n",
    "                 .get_feature_names_out().tolist()))\n",
    "\n",
    "coefs = pd.DataFrame(\n",
    "    np.squeeze(pipe_lr_feat_eng.named_steps[\"logisticregression\"].coef_),\n",
    "    index=column_names,\n",
    "    columns=[\"Coefficients\"]\n",
    ")\n",
    "\n",
    "coefs[\"abs_coef\"] = np.abs(coefs[\"Coefficients\"])\n",
    "coefs = coefs.sort_values(by=\"abs_coef\", ascending=False)\n",
    "\n",
    "pd.DataFrame(coefs[:10][\"Coefficients\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Some of the coefficients are expected because they are closely related to disasters, for instance, the words survived, died, rescued are all very grave and are likely to be found in tweets that refer to an actual disaster. This applies to the other features with large coefficients as well. However, there are some words like ukrainian that have a large coefficient which is a bit strange, but it is possible that in the data set, tweets containing the word ukrainian are strongly associated with actual disasters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>average_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.890941</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>0.71754</td>\n",
       "      <td>0.924916</td>\n",
       "      <td>0.784924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision    recall  f1_score   roc_auc  average_precision\n",
       "0  0.890941   0.677419  0.762712   0.71754  0.924916           0.784924"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = pipe_lr_feat_eng.predict(X_test_eng)\n",
    "soft_preds = pipe_lr_feat_eng.predict_proba(X_test_eng)\n",
    "\n",
    "test_results = pd.DataFrame({\n",
    "    \"accuracy\": [pipe_lr_feat_eng.score(X_test_eng, y_test_eng)],\n",
    "    \"precision\": [precision_score(y_test_eng, preds)],\n",
    "    \"recall\": [recall_score(y_test_eng, preds)],\n",
    "    \"f1_score\": [f1_score(y_test_eng, preds)],\n",
    "    \"roc_auc\": [roc_auc_score(y_test_eng, soft_preds[:, 1])],\n",
    "    \"average_precision\": [average_precision_score(y_test_eng, soft_preds[:, 1])]\n",
    "})\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The test scores are similar to the scores obtained from cross-validation which suggests that our model has **good generalisation** and it is **not overfitting** to the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:573]",
   "language": "python",
   "name": "conda-env-573-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
